/**
 * AI Tutor Service
 * S·ª≠ d·ª•ng Cerebras GPT-OSS-120B v·ªõi Streaming
 * Fallback sang Groq khi Cerebras b·ªã rate limit
 *
 * NGUY√äN T·∫ÆC V√ÄNG: KH√îNG BAO GI·ªú VI·∫æT CODE CHO SINH VI√äN
 */

const Cerebras = require("@cerebras/cerebras_cloud_sdk").default;
const Groq = require("groq-sdk");
const { AITutorConversation } = require("../models");

class AITutorService {
  constructor() {
    this.cerebras = new Cerebras({ apiKey: process.env.CEREBRAS_API_KEY });
    this.groq = new Groq({ apiKey: process.env.GROQ_API_KEY });
    this.modelName = "gpt-oss-120b";
    this.groqModelName = "openai/gpt-oss-120b";
    this.conversationHistory = new Map();
    this.MAX_HISTORY_LENGTH = 20;
  }

  _isRateLimitError(error) {
    return (
      error.status === 429 ||
      error.message?.includes("429") ||
      error.message?.includes("rate") ||
      error.message?.includes("quota") ||
      error.message?.includes("Rate limit")
    );
  }

  // Groq completion v·ªõi c·∫•u h√¨nh chu·∫©n
  async _createGroqCompletion(messages, useStream = false, useJson = false) {
    const options = {
      messages,
      model: this.groqModelName,
      temperature: 1,
      max_completion_tokens: 65536,
      top_p: 1,
      stream: useStream,
      reasoning_effort: "high",
      stop: null,
    };
    if (useJson) {
      options.response_format = { type: "json_object" };
    }
    return await this.groq.chat.completions.create(options);
  }

  // Lu√¥n th·ª≠ Cerebras tr∆∞·ªõc, n·∫øu rate limit th√¨ fallback Groq
  async _createCompletion(options, useStream = false) {
    const messages = options.messages;
    const useJson = !!options.response_format;

    try {
      return await this.cerebras.chat.completions.create({
        messages,
        model: this.modelName,
        max_completion_tokens: options.max_completion_tokens || 8192,
        temperature: options.temperature || 1,
        top_p: options.top_p || 1,
        reasoning_effort: options.reasoning_effort || "high",
        stream: useStream,
        ...(options.response_format && {
          response_format: options.response_format,
        }),
      });
    } catch (error) {
      if (this._isRateLimitError(error)) {
        console.log("[AITutorService] Cerebras rate limited, fallback to Groq");
        return await this._createGroqCompletion(messages, useStream, useJson);
      }
      throw error;
    }
  }

  _getSystemPrompt(context = {}) {
    const { questionText, language, currentCode, testResults } = context;

    let systemPrompt = `B·∫°n l√† AI Tutor - tr·ª£ l√Ω h·ªçc l·∫≠p tr√¨nh th√¢n thi·ªán cho sinh vi√™n Vi·ªát Nam.

üéØ NHI·ªÜM V·ª§: Gi√∫p sinh vi√™n HI·ªÇU v√† T·ª∞ GI·∫¢I ƒë∆∞·ª£c b√†i t·∫≠p

‚ö†Ô∏è NGUY√äN T·∫ÆC V√ÄNG:
1. KH√îNG BAO GI·ªú vi·∫øt code ho√†n ch·ªânh
2. KH√îNG ƒë∆∞a ra l·ªùi gi·∫£i tr·ª±c ti·∫øp
3. KH√îNG vi·∫øt h√†m/function c√≥ th·ªÉ copy-paste
4. Ch·ªâ d√πng v√≠ d·ª• KH√ÅC ho√†n to√†n v·ªõi b√†i t·∫≠p

‚úÖ THAY V√ÄO ƒê√ì:
- ƒê·∫∑t c√¢u h·ªèi g·ª£i m·ªü
- Gi·∫£i th√≠ch b·∫±ng v√≠ d·ª• ƒë·ªùi th∆∞·ªùng
- H∆∞·ªõng d·∫´n t·ª´ng b∆∞·ªõc logic
- Khuy·∫øn kh√≠ch v√† ƒë·ªông vi√™n

üí¨ PHONG C√ÅCH: Th√¢n thi·ªán, d√πng emoji üòä, gi·∫£i th√≠ch ƒë∆°n gi·∫£n`;

    if (questionText) systemPrompt += `\n\nüìù ƒê·ªÅ b√†i: ${questionText}`;
    if (language) systemPrompt += `\nNg√¥n ng·ªØ: ${language.toUpperCase()}`;
    if (currentCode)
      systemPrompt += `\n\nCode hi·ªán t·∫°i:\n\`\`\`${
        language || "c"
      }\n${currentCode}\n\`\`\``;
    if (testResults) {
      systemPrompt += `\n\nK·∫øt qu·∫£ test: ${testResults.passed}/${testResults.total} passed`;
      if (testResults.error) systemPrompt += `\nL·ªói: ${testResults.error}`;
    }

    return systemPrompt;
  }

  async _getHistory(sessionId) {
    if (this.conversationHistory.has(sessionId)) {
      return this.conversationHistory.get(sessionId);
    }

    try {
      const dbHistory = await AITutorConversation.findAll({
        where: { session_id: sessionId },
        order: [["created_at", "ASC"]],
        limit: this.MAX_HISTORY_LENGTH,
      });

      if (dbHistory.length > 0) {
        const history = dbHistory.map((msg) => ({
          role: msg.role === "model" ? "assistant" : msg.role,
          content: msg.message,
        }));
        this.conversationHistory.set(sessionId, history);
        return history;
      }
    } catch (err) {
      console.error("[AITutorService] Error loading history:", err.message);
    }

    this.conversationHistory.set(sessionId, []);
    return [];
  }

  async _addToHistory(
    sessionId,
    role,
    content,
    userId = null,
    questionId = null,
    context = null
  ) {
    const history = await this._getHistory(sessionId);
    const cerebrasRole = role === "model" ? "assistant" : role;
    history.push({ role: cerebrasRole, content });

    if (history.length > this.MAX_HISTORY_LENGTH) {
      const trimmed = [
        ...history.slice(0, 2),
        ...history.slice(-(this.MAX_HISTORY_LENGTH - 2)),
      ];
      this.conversationHistory.set(sessionId, trimmed);
    }

    // Ch·ªâ l∆∞u v√†o DB n·∫øu c√≥ userId V√Ä kh√¥ng ph·∫£i system prompt
    const isSystemPrompt =
      role === "system" ||
      content.startsWith("B·∫°n l√† AI Tutor") ||
      content.includes("NHI·ªÜM V·ª§ CH√çNH:") ||
      content.includes("NGUY√äN T·∫ÆC V√ÄNG");

    if (userId && !isSystemPrompt) {
      this._saveToDatabase(
        sessionId,
        role,
        content,
        userId,
        questionId,
        context
      ).catch((err) =>
        console.error("[AITutorService] Error saving to DB:", err.message)
      );
    }
  }

  async _saveToDatabase(sessionId, role, message, userId, questionId, context) {
    try {
      await AITutorConversation.create({
        user_id: userId,
        question_id: questionId || null,
        session_id: sessionId,
        role: role,
        message: message,
        context_snapshot: context
          ? {
              language: context.language,
              has_code: !!context.currentCode,
              code_length: context.currentCode?.length || 0,
            }
          : null,
      });
    } catch (err) {
      console.error("[AITutorService] DB save error:", err.message);
    }
  }

  async clearHistory(sessionId, userId = null) {
    this.conversationHistory.delete(sessionId);
    if (userId) {
      try {
        await AITutorConversation.destroy({ where: { session_id: sessionId } });
      } catch (err) {
        console.error(
          "[AITutorService] Error clearing DB history:",
          err.message
        );
      }
    }
  }

  /**
   * Main chat function v·ªõi Streaming
   */
  async chat(
    sessionId,
    userMessage,
    context = {},
    userId = null,
    questionId = null
  ) {
    try {
      const history = await this._getHistory(sessionId);
      const systemPrompt = this._getSystemPrompt(context);

      // Initialize conversation if new
      if (history.length === 0) {
        const greeting =
          "Xin ch√†o! üëã M√¨nh l√† AI Tutor, tr·ª£ l√Ω h·ªçc l·∫≠p tr√¨nh c·ªßa b·∫°n. B·∫°n c·∫ßn h·ªó tr·ª£ g√¨ n√†o? üòä";
        history.push({ role: "user", content: systemPrompt });
        history.push({ role: "assistant", content: greeting });
        if (userId) {
          await this._saveToDatabase(
            sessionId,
            "user",
            systemPrompt,
            userId,
            questionId,
            context
          );
          await this._saveToDatabase(
            sessionId,
            "model",
            greeting,
            userId,
            questionId,
            null
          );
        }
      }

      await this._addToHistory(
        sessionId,
        "user",
        userMessage,
        userId,
        questionId,
        context
      );

      const messages = [
        { role: "system", content: systemPrompt },
        ...(await this._getHistory(sessionId)),
      ];

      // S·ª≠ d·ª•ng Streaming v·ªõi Cerebras, fallback sang Groq n·∫øu rate limit
      let aiMessage = "";
      const stream = await this._createCompletion(
        {
          messages,
          max_completion_tokens: 8192,
          temperature: 1,
          top_p: 1,
          reasoning_effort: "high",
        },
        true
      );

      for await (const chunk of stream) {
        aiMessage += chunk.choices[0]?.delta?.content || "";
      }

      if (!aiMessage) aiMessage = "Xin l·ªói, m√¨nh kh√¥ng th·ªÉ tr·∫£ l·ªùi l√∫c n√†y.";

      await this._addToHistory(
        sessionId,
        "model",
        aiMessage,
        userId,
        questionId,
        null
      );

      return {
        success: true,
        message: aiMessage,
        sessionId,
        historyLength: (await this._getHistory(sessionId)).length,
      };
    } catch (error) {
      console.error("[AITutorService] Chat error:", error);

      if (error.message?.includes("SAFETY")) {
        return {
          success: false,
          message: "Xin l·ªói, m√¨nh kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y. üôè",
          error: "safety_filter",
        };
      }
      if (error.message?.includes("quota") || error.message?.includes("429")) {
        return {
          success: false,
          message: "H·ªá th·ªëng ƒëang b·∫≠n, b·∫°n th·ª≠ l·∫°i sau v√†i gi√¢y nh√©! ‚è≥",
          error: "rate_limit",
        };
      }
      return {
        success: false,
        message: "C√≥ l·ªói x·∫£y ra, b·∫°n th·ª≠ l·∫°i nh√©! üòÖ",
        error: error.message,
      };
    }
  }

  /**
   * Chat v·ªõi Streaming Response (tr·∫£ v·ªÅ stream cho frontend)
   */
  async chatStream(
    sessionId,
    userMessage,
    context = {},
    userId = null,
    questionId = null
  ) {
    const history = await this._getHistory(sessionId);
    const systemPrompt = this._getSystemPrompt(context);

    if (history.length === 0) {
      const greeting =
        "Xin ch√†o! üëã M√¨nh l√† AI Tutor. B·∫°n c·∫ßn h·ªó tr·ª£ g√¨ n√†o? üòä";
      history.push({ role: "user", content: systemPrompt });
      history.push({ role: "assistant", content: greeting });
      if (userId) {
        await this._saveToDatabase(
          sessionId,
          "user",
          systemPrompt,
          userId,
          questionId,
          context
        );
        await this._saveToDatabase(
          sessionId,
          "model",
          greeting,
          userId,
          questionId,
          null
        );
      }
    }

    await this._addToHistory(
      sessionId,
      "user",
      userMessage,
      userId,
      questionId,
      context
    );

    const messages = [
      { role: "system", content: systemPrompt },
      ...(await this._getHistory(sessionId)),
    ];

    // Return stream directly for SSE, with Groq fallback
    const stream = await this._createCompletion(
      {
        messages,
        max_completion_tokens: 8192,
        temperature: 1,
        top_p: 1,
        reasoning_effort: "high",
      },
      true
    );

    return {
      stream,
      saveResponse: async (fullMessage) => {
        await this._addToHistory(
          sessionId,
          "model",
          fullMessage,
          userId,
          questionId,
          null
        );
      },
    };
  }

  /**
   * Quick help v·ªõi Streaming
   */
  async quickHelp(question, context = {}) {
    try {
      const systemPrompt = this._getSystemPrompt(context);
      const prompt = `${systemPrompt}\n\nC√¢u h·ªèi: ${question}\n\nTr·∫£ l·ªùi ng·∫Øn g·ªçn (d∆∞·ªõi 200 t·ª´):`;

      const messages = [
        { role: "system", content: "" },
        { role: "user", content: prompt },
      ];

      let message = "";
      const stream = await this._createCompletion(
        {
          messages,
          max_completion_tokens: 2048,
          temperature: 1,
          top_p: 1,
          reasoning_effort: "high",
        },
        true
      );

      for await (const chunk of stream) {
        message += chunk.choices[0]?.delta?.content || "";
      }

      return { success: true, message: message || "Kh√¥ng th·ªÉ tr·∫£ l·ªùi." };
    } catch (error) {
      console.error("[AITutorService] Quick help error:", error);
      return {
        success: false,
        message: "C√≥ l·ªói x·∫£y ra, b·∫°n th·ª≠ l·∫°i nh√©!",
        error: error.message,
      };
    }
  }

  /**
   * Explain concept v·ªõi Streaming
   */
  async explainConcept(concept, language = "c") {
    try {
      const prompt = `Gi·∫£i th√≠ch kh√°i ni·ªám "${concept}" trong ${language.toUpperCase()} cho sinh vi√™n m·ªõi h·ªçc.
Y√™u c·∫ßu: ƒê∆°n gi·∫£n, d√πng v√≠ d·ª• th·ª±c t·∫ø (KH√îNG code), t·ªëi ƒëa 300 t·ª´, d√πng emoji.`;

      const stream = await this._createCompletion(
        {
          messages: [
            {
              role: "system",
              content: "B·∫°n l√† gi√°o vi√™n l·∫≠p tr√¨nh th√¢n thi·ªán.",
            },
            { role: "user", content: prompt },
          ],
          max_completion_tokens: 2048,
          temperature: 1,
          top_p: 1,
          reasoning_effort: "high",
        },
        true
      );

      let explanation = "";
      for await (const chunk of stream) {
        explanation += chunk.choices[0]?.delta?.content || "";
      }

      return {
        success: true,
        concept,
        explanation: explanation || "Kh√¥ng th·ªÉ gi·∫£i th√≠ch.",
      };
    } catch (error) {
      console.error("[AITutorService] Explain concept error:", error);
      return { success: false, error: error.message };
    }
  }

  /**
   * Get hint v·ªõi Streaming
   */
  async getHint(questionText, currentCode, language, hintLevel = 1) {
    try {
      const hintLevelDesc = {
        1: "G·ª£i √Ω r·∫•t nh·∫π - ch·ªâ h∆∞·ªõng suy nghƒ© chung",
        2: "G·ª£i √Ω trung b√¨nh - ch·ªâ ra v·∫•n ƒë·ªÅ c·ª• th·ªÉ h∆°n",
        3: "G·ª£i √Ω chi ti·∫øt - h∆∞·ªõng d·∫´n t·ª´ng b∆∞·ªõc logic (v·∫´n KH√îNG cho code)",
      };

      const prompt = `üìù ƒê·ªÅ b√†i: ${questionText}

üíª Code hi·ªán t·∫°i:
\`\`\`${language}
${currentCode}
\`\`\`

üéØ M·ª©c g·ª£i √Ω: ${hintLevel}/3 - ${hintLevelDesc[hintLevel] || hintLevelDesc[1]}

‚ö†Ô∏è TUY·ªÜT ƒê·ªêI KH√îNG vi·∫øt code, KH√îNG cho l·ªùi gi·∫£i tr·ª±c ti·∫øp!`;

      const stream = await this._createCompletion(
        {
          messages: [
            {
              role: "system",
              content: "B·∫°n l√† AI Tutor. Ch·ªâ g·ª£i √Ω h∆∞·ªõng suy nghƒ©.",
            },
            { role: "user", content: prompt },
          ],
          max_completion_tokens: 2048,
          temperature: 1,
          top_p: 1,
          reasoning_effort: "high",
        },
        true
      );

      let hint = "";
      for await (const chunk of stream) {
        hint += chunk.choices[0]?.delta?.content || "";
      }

      return {
        success: true,
        hintLevel,
        hint: hint || "Kh√¥ng th·ªÉ t·∫°o g·ª£i √Ω.",
        nextHintAvailable: hintLevel < 3,
      };
    } catch (error) {
      console.error("[AITutorService] Get hint error:", error);
      return { success: false, error: error.message };
    }
  }

  /**
   * Review code v·ªõi Streaming
   */
  async reviewCode(code, language, questionText = null) {
    try {
      let prompt = `üíª Code:\n\`\`\`${language}\n${code}\n\`\`\``;
      if (questionText) prompt += `\n\nüìù ƒê·ªÅ b√†i: ${questionText}`;
      prompt += `\n\nReview v√† nh·∫≠n x√©t:
1. ‚úÖ ƒêi·ªÉm t·ªët
2. ‚ö†Ô∏è V·∫•n ƒë·ªÅ c·∫ßn c·∫£i thi·ªán
3. üí° G·ª£i √Ω h∆∞·ªõng c·∫£i thi·ªán (KH√îNG cho code)
4. üìö Kh√°i ni·ªám n√™n √¥n l·∫°i

‚ö†Ô∏è TUY·ªÜT ƒê·ªêI KH√îNG vi·∫øt code s·ª≠a! (t·ªëi ƒëa 250 t·ª´)`;

      const stream = await this._createCompletion(
        {
          messages: [
            { role: "system", content: "B·∫°n l√† AI Tutor ƒëang review code." },
            { role: "user", content: prompt },
          ],
          max_completion_tokens: 2048,
          temperature: 1,
          top_p: 1,
          reasoning_effort: "high",
        },
        true
      );

      let review = "";
      for await (const chunk of stream) {
        review += chunk.choices[0]?.delta?.content || "";
      }

      return { success: true, review: review || "Kh√¥ng th·ªÉ review." };
    } catch (error) {
      console.error("[AITutorService] Review code error:", error);
      return { success: false, error: error.message };
    }
  }

  async getSessionStats(sessionId, userId = null) {
    const history = await this._getHistory(sessionId);
    let dbCount = 0;
    if (userId) {
      try {
        dbCount = await AITutorConversation.count({
          where: { session_id: sessionId },
        });
      } catch (err) {
        /* ignore */
      }
    }
    return {
      sessionId,
      messageCount: history.length,
      dbMessageCount: dbCount,
      isActive: history.length > 0,
    };
  }

  async getConversationHistory(userId, questionId = null, limit = 50) {
    try {
      const where = { user_id: userId };
      if (questionId) where.question_id = questionId;
      const messages = await AITutorConversation.findAll({
        where,
        order: [["created_at", "DESC"]],
        limit: limit * 2, // L·∫•y nhi·ªÅu h∆°n ƒë·ªÉ filter
        attributes: ["id", "session_id", "role", "message", "created_at"],
      });

      // Filter b·ªè system messages v√† c√°c tin nh·∫Øn ch·ª©a system prompt
      const filtered = messages.filter((msg) => {
        // B·ªè qua role system
        if (msg.role === "system") return false;

        // B·ªè qua c√°c tin nh·∫Øn ch·ª©a system prompt (th∆∞·ªùng b·∫Øt ƒë·∫ßu b·∫±ng "B·∫°n l√† AI Tutor")
        const content = msg.message || "";
        if (
          content.startsWith("B·∫°n l√† AI Tutor") ||
          content.includes("NHI·ªÜM V·ª§ CH√çNH:") ||
          content.includes("NGUY√äN T·∫ÆC V√ÄNG") ||
          content.includes("TUY·ªÜT ƒê·ªêI TU√ÇN TH·ª¶")
        ) {
          return false;
        }

        return true;
      });

      // Gi·ªõi h·∫°n l·∫°i s·ªë l∆∞·ª£ng v√† ƒë·∫£o ng∆∞·ª£c ƒë·ªÉ c√≥ th·ª© t·ª± ƒë√∫ng
      return filtered.slice(-limit).reverse();
    } catch (err) {
      console.error("[AITutorService] Error getting history:", err.message);
      return [];
    }
  }

  cleanupOldSessions() {
    if (this.conversationHistory.size > 1000) {
      const entries = Array.from(this.conversationHistory.entries());
      this.conversationHistory = new Map(entries.slice(-500));
    }
  }
}

module.exports = AITutorService;
